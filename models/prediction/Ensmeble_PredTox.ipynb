{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensmeble-PredTox.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjkE0ukhf-77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize drive|\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytg4kXDbgOqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move to Google Drive \n",
        "%cd drive\n",
        "%cd 'My Drive'\n",
        "%cd 'MSc Stats Dissertation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEc2JPwChQnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Install necessary libraries\n",
        "!pip install deepsmiles\n",
        "!pip install selfies\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QROpfvk9gP9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Go to correct place  in drive to allow us \n",
        "## to import libraries\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2YH5nGJgRde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import Necessary lIbraries \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "import math\n",
        "import tensorflow.keras.layers as layers\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import deepsmiles\n",
        "from selfies import encoder, decoder  \n",
        "import rdkit\n",
        "import Utils.generate_utils as generate_utils\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.under_sampling import RandomUnderSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7BbBLSfhbn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Converter to convert SMILES to Deep SMILES\n",
        "converter = deepsmiles.Converter(rings = True, branches = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA4yAsDZgUWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set to correct float type for consistency with training\n",
        "tf.keras.backend.set_floatx('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfPsMwnsgdvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import data files\n",
        "SELFIES = False\n",
        "DEEP = False\n",
        "train_smiles_path = './Datasets/train_Tox_data.smi'\n",
        "test_smiles_path = './Datasets/AID_1189_datatable_all.csv'\n",
        "actual_test_smiles_path = './Datasets/3643044675069416146.txt'\n",
        "if SELFIES:\n",
        "  vocab =np.load('./vocab/selfies_vocab.npy',allow_pickle=True)\n",
        "  vocab_index = np.load('./vocab/selfies_vocab_index.npy',allow_pickle=True)\n",
        "elif DEEP:\n",
        "  vocab =np.load('./vocab/deep_vocab.npy',allow_pickle=True)\n",
        "  vocab_index = np.load('./vocab/deep_vocab_index.npy',allow_pickle=True)\n",
        "else:\n",
        "  vocab =np.load('./vocab/vocab.npy',allow_pickle=True)\n",
        "  vocab_index = np.load('./vocab/vocab_index.npy',allow_pickle=True)\n",
        "vocab = dict(vocab.ravel()[0])\n",
        "vocab_index = dict(vocab_index.ravel()[0])\n",
        "smiles_train =  pd.read_csv(train_smiles_path,delimiter='\\t',header=None)\n",
        "\n",
        "MIN = 3027"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsKb73tVeOur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load in file and process it for later predcitions\n",
        "smiles_test =  pd.read_csv(test_smiles_path,delimiter=',')\n",
        "actual_test_smiles = pd.read_csv(actual_test_smiles_path,delimiter='\\t',header = None)\n",
        "train =[]\n",
        "test = []\n",
        "train_Y = []\n",
        "test_Y = []\n",
        "for index in range(len(smiles_train[1])):\n",
        "  if smiles_train[1][index][:2] =='DB':\n",
        "    test.append(index)\n",
        "    test_Y.append(0)\n",
        "  elif smiles_train[1][index][:1] =='D':\n",
        "    train.append(index)\n",
        "    train_Y.append(0)\n",
        "  elif smiles_train[1][index][:2] =='T3':\n",
        "    test.append(index)\n",
        "    test_Y.append(1)\n",
        "  else:\n",
        "    train.append(index)\n",
        "    train_Y.append(1)\n",
        "\n",
        "train_X = smiles_train[0][train]\n",
        "test_X = smiles_train[0][test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFZd4Al-g6ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Neccesary CONSTANTS\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE =  0.000312087049936\n",
        "if SELFIES or DEEP:\n",
        "  PAD_LEN = 250\n",
        "else:\n",
        "  PAD_LEN = 160 ## Maximum size of a SMILE (100 + BOS, EOS)\n",
        "  print('HERE')\n",
        "MAX_LEN = PAD_LEN \n",
        "DROP_OUT= 0.2\n",
        "EMBEDDING_DIM = 192  ## Embedding dim of the characters\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT = 0.2\n",
        "TRAIN = False\n",
        "LATENT_DIM = 64\n",
        "TRANSFORMER_DECODE = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUfritPdN0v0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Integer encode for selfies\n",
        "def integer_encode_selfies(selfies,vocab_dict):\n",
        "  selfies_enc = []\n",
        "  for char in selfies:\n",
        "    try:\n",
        "      selfies_enc.append(vocab_dict[char])\n",
        "    except:\n",
        "      return None\n",
        "  return selfies_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz1Lti6YN17r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Splits the selfies <molecule> into a list of character strings.\n",
        "def split_selfie(molecule):\n",
        "  return re.findall(r'\\[.*?\\]|\\.', molecule)\n",
        "\n",
        "## Takes processed selfies smiles and returns the tokenized \n",
        "## versions of the selfies\n",
        "def tokenize_selfies(selfies):\n",
        "  char_list = split_selfie(selfies)\n",
        "  tokenized= []\n",
        "  tokenized.append('<BOS>')\n",
        "  i = 0 \n",
        "  while i < len(char_list):\n",
        "    char = char_list[i]\n",
        "    tokenized.append(char)\n",
        "    i = i+1\n",
        "  tokenized.append('<EOS>')\n",
        "  return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weL_xZbaHC_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "## replace Br and Cl with single letters\n",
        "def replace_halogens(string):\n",
        "  br = re.compile('Br')\n",
        "  cl = re.compile('Cl')\n",
        "  string = br.sub('R', string)\n",
        "  string = cl.sub('L', string)\n",
        "  return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWBy97fvHIPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Takes processed smiles/deep smiles and returns the tokenized \n",
        "## versions of the smiles or deep semiles\n",
        "## Note: Run replace halogens and replace percentages\n",
        "## before running this method \n",
        "def tokenize_smiles(smiles):\n",
        "  char_list = list(smiles)\n",
        "  tokenized= []\n",
        "  tokenized.append('<BOS>')\n",
        "  i = 0 \n",
        "  while i < len(char_list):\n",
        "    char = char_list[i]\n",
        "    tokenized.append(char)\n",
        "    i= i+1\n",
        "  tokenized.append('<EOS>')\n",
        "  return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQtW4ehhHGIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Integer encode fore SMILES and DeepSMILES\n",
        "def integer_encode(smiles,vocab_dict):\n",
        "  smiles_enc = []\n",
        "  for char in smiles:\n",
        "    if char in vocab:\n",
        "      smiles_enc.append(vocab_dict[char])\n",
        "    else:\n",
        "       return None\n",
        "  return smiles_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ5ExLgxHOJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Process for later prediction\n",
        "smile_pair_tokens = []\n",
        "indexes = []\n",
        "index = 0 \n",
        "for smiles in train_X:\n",
        "  if SELFIES:\n",
        "    smiles = encoder(smiles)\n",
        "    if smiles is not None:\n",
        "      indexes.append(index)\n",
        "      smile_pair_tokens.append(tokenize_selfies(smiles))\n",
        "  elif DEEP:\n",
        "    smiles = replace_halogens(smiles)\n",
        "    smile_pair_tokens.append(tokenize_smiles(converter.encode(smiles)))\n",
        "  else:\n",
        "    smiles = replace_halogens(smiles)\n",
        "    smile_pair_tokens.append(tokenize_smiles(smiles))\n",
        "  index = index+1\n",
        "smile_pair_tokens = np.array(smile_pair_tokens)\n",
        "if SELFIES:\n",
        "  train_Y = np.array(train_Y)[indexes]\n",
        "\n",
        "\n",
        "smiles_ordered = tf.keras.preprocessing.sequence.pad_sequences(smiles_ordered,maxlen = PAD_LEN,padding='post')\n",
        "tox_smiles = np.array(smiles_ordered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFpTzkvtHZaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Prrocess for later prediction \n",
        "smiles_ordered = []\n",
        "indexes = []\n",
        "index = 0 \n",
        "for smiles in smile_pair_tokens:\n",
        "  if SELFIES:\n",
        "    encoded_smile = integer_encode_selfies(smiles,vocab)\n",
        "    if encoded_smile is not None:\n",
        "      smiles_ordered.append(encoded_smile)\n",
        "    else:\n",
        "      smiles_ordered.append(None)\n",
        "  else:\n",
        "    smiles_ordered.append(integer_encode(smiles,vocab))\n",
        "l=[i for i,v in enumerate(smiles_ordered) if v != None ]\n",
        "smiles_ordered = np.array(smiles_ordered)[l]\n",
        "train_Y  = np.array(train_Y)[l]\n",
        "l=[i for i,v in enumerate(smiles_ordered) if len(v) <MAX_LEN]\n",
        "smiles_ordered = np.array(smiles_ordered)[l]\n",
        "train_Y  = train_Y[l]\n",
        "\n",
        "\n",
        "\n",
        "smile_pair_tokens = []\n",
        "indexes = []\n",
        "index = 0 \n",
        "for smiles in test_X:\n",
        "  if SELFIES:\n",
        "    smiles = encoder(smiles)\n",
        "    if smiles is not None:\n",
        "      indexes.append(index)\n",
        "      smile_pair_tokens.append(tokenize_selfies(smiles))\n",
        "  elif DEEP:\n",
        "    smiles = replace_halogens(smiles)\n",
        "    smile_pair_tokens.append(tokenize_smiles(converter.encode(smiles)))\n",
        "  else:\n",
        "    smiles = replace_halogens(smiles)\n",
        "    smile_pair_tokens.append(tokenize_smiles(smiles))\n",
        "  index = index+1\n",
        "smile_pair_tokens = np.array(smile_pair_tokens)\n",
        "if SELFIES:\n",
        "  test_Y = np.array(test_Y)[indexes]\n",
        "\n",
        "\n",
        "smiles_ordered = []\n",
        "for smiles in smile_pair_tokens:\n",
        "  if SELFIES:\n",
        "    encoded_smile = integer_encode_selfies(smiles,vocab)\n",
        "    smiles_ordered.append(encoded_smile)\n",
        "  else:\n",
        "    smiles_ordered.append(integer_encode(smiles,vocab))\n",
        "l=[i for i,v in enumerate(smiles_ordered) if v != None ]\n",
        "smiles_ordered = np.array(smiles_ordered)[l]\n",
        "test_Y  = np.array(test_Y)[l]\n",
        "l=[i for i,v in enumerate(smiles_ordered) if len(v) <MAX_LEN]\n",
        "cancer_smiles = np.array(smiles_ordered)[l]\n",
        "test_Y  = test_Y[l]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pv8ZFiyq-ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import GANS.renewed_smiles_vae as conv_smiles_vae\n",
        "import GANS.ic50vae as ic50vae\n",
        "\n",
        "## Import in the VAE so that embeddings of different compounds can be calculated\n",
        "if IC50:\n",
        "  smile_vae = ic50vae.SMILE_VAE(vocab_size= VOCAB_SIZE,embedding_dim=EMBEDDING_DIM, max_len= MAX_LEN,\n",
        "                      latent_dim = LATENT_DIM, recurrent_dropout = DROP_OUT,dropout_rate= DROP_OUT)\n",
        "  if DEEP:\n",
        "    print('IC50 DEEP')\n",
        "    smile_vae.load_weights('ic50g_deep_conv_vae_weights')\n",
        "  elif SELFIES:\n",
        "    print('IC50 SELFIES')\n",
        "    smile_vae.load_weights('ic50g_selfies_conv_vae_weights')\n",
        "  else:\n",
        "    print('IC50 NORMAL')\n",
        "    smile_vae.load_weights('ic50g_smiles_conv_vae_weights')\n",
        "else:\n",
        "  smile_vae = conv_smiles_vae.SMILE_VAE(vocab_size= VOCAB_SIZE,embedding_dim=EMBEDDING_DIM, max_len= MAX_LEN, \n",
        "                      latent_dim = LATENT_DIM, recurrent_dropout = DROP_OUT,dropout_rate= DROP_OUT)\n",
        "  if DEEP:\n",
        "    print('DEEP')\n",
        "    smile_vae.load_weights('deep_conv_vae_weights2')\n",
        "  elif SELFIES:\n",
        "    print('SELFIES')\n",
        "    smile_vae.load_weights('selfies_conv_vae_weights2')\n",
        "  else:\n",
        "    print('NORMAL')\n",
        "    smile_vae.load_weights('smiles_conv_vae_weights2')\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmlDZR5wrCA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get the latents representations for the training data\n",
        "train_latents = []\n",
        "index = 0 \n",
        "for smile in smiles_ordered:\n",
        "  train_latents.append(smile_vae.encoder(smile.reshape(1,MAX_LEN))[1])\n",
        "  if index %1000 == 0:\n",
        "    print(index)\n",
        "  index+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2sG_dRMis1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get test representations for the test data \n",
        "cancer_smiles = tf.keras.preprocessing.sequence.pad_sequences(cancer_smiles,maxlen = PAD_LEN,padding='post')\n",
        "cancer_smiles = np.array(cancer_smiles)\n",
        "test_latents = []\n",
        "index = 0 \n",
        "for smile in cancer_smiles:\n",
        "  test_latents.append(smile_vae.encoder(smile.reshape(1,MAX_LEN))[1])\n",
        "  if index %100 == 0:\n",
        "    print(index)\n",
        "  index+=1\n",
        "test_latents = np.array(test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8_Ko3qwJYVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model,train_X,train_Y,test_X,test_Y):\n",
        "  model.fit(train_X,train_Y.ravel())\n",
        "  predictions = model.predict(test_X)\n",
        "  probs = model.predict_proba(test_X)\n",
        "  print(classification_report(test_Y, predictions))\n",
        "  print(confusion_matrix(test_Y, predictions))\n",
        "  print(accuracy_score(test_Y, predictions))\n",
        "  print(matthews_corrcoef(test_Y, predictions))\n",
        "  return predictions, probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYaM8kMEic-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_on_model(model,test_X,test_Y):\n",
        "  predictions = model.predict(test_X)\n",
        "  probs = model.predict_proba(test_X)\n",
        "  print(classification_report(test_Y, predictions))\n",
        "  print(confusion_matrix(test_Y, predictions))\n",
        "  print(accuracy_score(test_Y, predictions))\n",
        "  print(matthews_corrcoef(test_Y, predictions))\n",
        "  return predictions, probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaaCdDJeanni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mcc_curve(y_true, predictions_prob):\n",
        "  cutoffs = np.arange(0,1,1e-4)\n",
        "  mccs = []\n",
        "  for cutoff in cutoffs:\n",
        "    predictions = labels = (predictions_prob > cutoff).astype(np.int)\n",
        "    mccs.append(matthews_corrcoef(y_true, predictions))\n",
        "  return cutoffs, mccs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4uxo1h0j8jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "## plot ROC curve\n",
        "def plot_roc_curve(y_true, y_probs, title):\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_probs, pos_label=1)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    matplotlib.rc('font', size=20)\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='black',\n",
        "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(\"./Figures/\" + title + \".pdf\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## plot MCC curve\n",
        "def plot_mcc_curve(y_true, y_probs,title ):\n",
        "  cuttofs, mccs = get_mcc_curve(y_true,y_probs)\n",
        "  matplotlib.rc('font', size=20)\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  lw = 2\n",
        "  plt.plot(cuttofs, mccs, color='black',\n",
        "            lw=lw)\n",
        "  #plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 0.41])\n",
        "  plt.xlabel('Toxicity Probability Cutoff')\n",
        "  plt.ylabel('MCC')\n",
        "  #plt.legend(loc=\"lower right\")\n",
        "  fig.tight_layout()\n",
        "  fig.savefig(\"./Figures/\" + title + \".pdf\", bbox_inches='tight')\n",
        "  plt.show()\n",
        "\n",
        "## plot Confusion Matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, title,\n",
        "                          normalize=False, cmap=plt.cm.Blues):\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    # classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    matplotlib.rc('font', size=20)\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "    _ = plt.xlabel(\"Predicted Labels\", fontsize=18)\n",
        "    _ = plt.ylabel(\"True label\", fontsize=18)\n",
        "\n",
        "    plt.rc('xtick', labelsize=14)\n",
        "    plt.rc('ytick', labelsize=14)\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\", fontsize=14,\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(\"./Figures/\" + title + \".pdf\", bbox_inches='tight')\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMh-IREutJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_latents = np.array(train_latents).reshape(-1,64)#\n",
        "test_latents = np.array(test_latents).reshape(-1,64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RrjMZyzkCpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Parameters for performing grid searches\n",
        "params = {\n",
        "        'boosting_type': 'dart',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.85,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0,\n",
        "    }\n",
        "\n",
        "param_grid ={'boosting_type': ['dart','lgbm'],\n",
        "             'objective': ['binary'],\n",
        "             'n_estimators': [1200,1400,1600,1800],\n",
        "             'learning_rate': [0.1,0.05],\n",
        "             'feature_fraction': [0.9,1.0],\n",
        "             'num_leaves' :[31,63,127,255,511,800],\n",
        "             'lambda_l2 ':[0.0,0.1,0.5,1,5]\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywwUcccj285F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make an ensemble prediction for classification\n",
        "def ensemble_probabilities(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict_proba(testX)[:,1] for model in members]\n",
        "\tyhats = np.array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = np.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = summed/len(members)\n",
        "\treturn np.reshape(result,(len(result),1))\n",
        "def ensemble_predictions(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict(testX) for model in members]\n",
        "\tyhats = np.array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = np.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = summed/len(members)\n",
        "\treturn np.reshape(result,(len(result),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj0XLVz-4l3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def TPR(y_true, y_pred):\n",
        "    # counts the number of true positives (y_true = 1, y_pred = 1)\n",
        "    tp = list((y_true == 1) & (y_pred == 1)).count(True)\n",
        "    n = list((y_true == 1)).count(True)\n",
        "    return tp/n\n",
        "def FNR(y_true, y_pred):\n",
        "    # counts the number of false negatives (y_true = 1, y_pred = 0)\n",
        "    fn = list((y_true == 1) & (y_pred == 0)).count(True)\n",
        "    p = list(y_true == 1).count(True)\n",
        "    return fn/p\n",
        "def FPR(y_true, y_pred):\n",
        "    # counts the number of false positives (y_true = 0, y_pred = 1)\n",
        "    fp = list((y_true == 0) & (y_pred == 1)).count(True)\n",
        "    n = list(y_true == 0).count(True)\n",
        "    return fp/n\n",
        "def TNR(y_true, y_pred):\n",
        "    # counts the number of true negatives (y_true = 0, y_pred = 0)\n",
        "    tn= list((y_true == 0) & (y_pred == 0)).count(True)\n",
        "    n = list(y_true == 0).count(True)\n",
        "    return tn/n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhCjvyjY7BUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Found parameters for the DART and LGBM models\n",
        "params_lgbm = {\n",
        "        'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 1.0,\n",
        "    'num_leaves':63,\n",
        "    'verbose': 1,\n",
        "    'min_data_in_leaf':10,\n",
        "    ' n_estimators' : 2000,\n",
        "    }\n",
        "  \n",
        "\n",
        "params_dart = {\n",
        "      'boosting_type': 'dart',\n",
        "  'objective': 'binary',\n",
        "  'metric': 'binary_logloss',\n",
        "  'learning_rate': 0.05,\n",
        "  'feature_fraction': 1.0,\n",
        "  'num_leaves':127,\n",
        "  'verbose': 1,\n",
        "  'min_data_in_leaf':10,\n",
        "  ' n_estimators' : 2000,\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l31vRJZMpN1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "param_grid ={'max_depth': [10, 20, 30, 40, 50,60,70,80,90,100],\n",
        "             'n_estimators': [100,200, 400, 600, 800, 1000, 1200]}\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N534_24I2eq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "def get_nnmodel():\n",
        "  my_init = keras.initializers.glorot_uniform(seed=1)\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(units=4096, input_dim=64,\n",
        "    activation='relu', kernel_initializer=my_init)) \n",
        "  model.add(keras.layers.Dropout(0.7))\n",
        "  model.add(keras.layers.Dense(units=2048, activation='relu',\n",
        "    kernel_initializer=my_init)) \n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(units=1024, activation='relu',\n",
        "    kernel_initializer=my_init))\n",
        "  model.add(keras.layers.Dropout(0.5)) \n",
        "  model.add(keras.layers.Dense(units=512, activation='relu',\n",
        "    kernel_initializer=my_init)) \n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(units=256, activation='relu',\n",
        "    kernel_initializer=my_init)) \n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(units=512, activation='relu',\n",
        "    kernel_initializer=my_init))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(units=1024, activation='relu',\n",
        "    kernel_initializer=my_init)) \n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(units=1, activation='sigmoid',\n",
        "      kernel_initializer=my_init))\n",
        "  simple_sgd = keras.optimizers.SGD(lr=0.1)  \n",
        "  model.compile(loss='binary_crossentropy',\n",
        "    optimizer=simple_sgd,  metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv1rTAjmQcQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_QDAmodel(train_X, train_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y.ravel())\n",
        "  y_rs = np.array(y_rs)\n",
        "  x_rs = np.array(x_rs)\n",
        "  QDA = QuadraticDiscriminantAnalysis()\n",
        "  QDA.fit(x_rs,y_rs)\n",
        "  return QDA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-5FprOM5DJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_LGBmodel(train_X, train_Y,test_X,test_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y)\n",
        "  lgb_train = lgb.Dataset(x_rs, y_rs)\n",
        "  lgb_eval = lgb.Dataset(test_X, test_Y.flatten(), reference=lgb_train)\n",
        "  model = lgb.train(params_lgbm, \n",
        "                    lgb_train, \n",
        "                    num_boost_round=2000,\n",
        "                    valid_sets=lgb_eval,\n",
        "                    early_stopping_rounds=100,\n",
        "                    verbose_eval=True)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_vkR8O85Exm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_DARTmodel(train_X, train_Y,test_X,test_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y)\n",
        "  lgb_train = lgb.Dataset(x_rs, y_rs)\n",
        "  lgb_eval = lgb.Dataset(test_X, test_Y.flatten(), reference=lgb_train)\n",
        "  model = lgb.train(params_dart, \n",
        "                    lgb_train, \n",
        "                    num_boost_round=100,\n",
        "                    valid_sets=lgb_eval,\n",
        "                    early_stopping_rounds=10,\n",
        "                    verbose_eval=True)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNx9QnG6X-38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_SVMmodel(train_X, train_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y.ravel())\n",
        "  y_rs = np.array(y_rs)\n",
        "  x_rs = np.array(x_rs)\n",
        "  SVM = SVC(probability = True)\n",
        "  SVM.fit(x_rs,y_rs)\n",
        "  return SVM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKSnJRoXR60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_LDAmodel(train_X, train_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y.ravel())\n",
        "  y_rs = np.array(y_rs)\n",
        "  x_rs = np.array(x_rs)\n",
        "  LDA = LinearDiscriminantAnalysis()\n",
        "  LDA.fit(x_rs,y_rs)\n",
        "  return LDA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f88I8Pxh3GQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "def fit_nnmodel(train_X, train_Y,test_X,test_Y):\n",
        "  global random_state;\n",
        "  random_state= random_state +1\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y)\n",
        "  model = get_nnmodel()\n",
        "  model.fit(x_rs, \n",
        "            y_rs,\n",
        "            batch_size=128,\n",
        "            shuffle=True, \n",
        "            validation_data=(test_X,test_Y),\n",
        "            epochs=10, \n",
        "            verbose=0)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397aCML2naq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "def fit_etmodel(train_X, train_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y)\n",
        "  etc = ExtraTreesClassifier(n_estimators=1000,max_depth=50)\n",
        "  etc.fit(train_X,train_Y)\n",
        "  return etc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLQhd4s_TANW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_rfmodel(train_X, train_Y):\n",
        "  global random_state;\n",
        "  sampler = RandomUnderSampler(ratio={0: MIN  , 1:MIN },random_state=random_state)\n",
        "  random_state= random_state +1\n",
        "  x_rs, y_rs = sampler.fit_sample(train_X, train_Y)\n",
        "  random_forest = RandomForestClassifier(n_estimators=1000,max_depth=30)\n",
        "  random_forest.fit(train_X,train_Y)\n",
        "  return random_forest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vg2skBkTBcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 1\n",
        "random_state= 0 \n",
        "members_rf = [fit_rfmodel(train_latents, train_Y) for _ in range(n_members)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDSeCfnaYaQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_rf = ensemble_predictions(members_rf,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFB1WKVcQq9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 10\n",
        "random_state= 0 \n",
        "members_nn = [fit_nnmodel(train_latents, train_Y,test_latents,test_Y) for _ in range(n_members)]\n",
        "predictions_nn = ensemble_predictions(members_nn,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JP3kuxmXfQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 10\n",
        "random_state= 0 \n",
        "members_lda = [fit_LDAmodel(train_latents, train_Y) for _ in range(n_members)]\n",
        "predictions_lda = ensemble_predictions(members_lda,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-uXfU_x2xpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 10\n",
        "random_state= 0 \n",
        "members_qda = [fit_QDAmodel(train_latents, train_Y) for _ in range(n_members)]\n",
        "predictions_qda = ensemble_predictions(members_qda,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3T0DZw621j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 10\n",
        "random_state = 0\n",
        "members_dart =[fit_DARTmodel(train_latents, train_Y,test_latents,test_Y) for _ in range(n_members)]\n",
        "predictions_dart = ensemble_predictions(members_dart,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbifkJj4PyCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 10\n",
        "random_state = 0\n",
        "members_lgbm =[fit_LGBmodel(train_latents, train_Y,test_latents,test_Y) for _ in range(n_members)]\n",
        "predictions_lgbm = ensemble_predictions(members_lgbm,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5Uzn89lYPyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 10\n",
        "random_state= 0 \n",
        "members_svm = [fit_SVMmodel(train_latents, train_Y) for _ in range(n_members)]\n",
        "predictions_svm= ensemble_predictions(members_svm,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1m7Uq0GoNLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_members = 1\n",
        "random_state= 0 \n",
        "members_etc = [fit_etmodel(train_latents, train_Y) for _ in range(n_members)]\n",
        "predictions_etc = ensemble_predictions(members_etc,test_latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqHRsvEYdGPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_models  = []\n",
        "all_models.append(members_lgbm)\n",
        "all_models.append(members_dart)\n",
        "all_models.append(members_qda)\n",
        "all_models.append(members_rf)\n",
        "all_models.append(members_nn)\n",
        "all_models.append(members_lda)\n",
        "all_models.append(members_svm)\n",
        "all_models.append(members_etc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH5TwMFCUNNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_probs = []\n",
        "all_probs.append(predictions_lgbm)\n",
        "all_probs.append(predictions_dart)\n",
        "all_probs.append(predictions_qda)\n",
        "all_probs.append(predictions_rf)\n",
        "all_probs.append(predictions_nn)\n",
        "#ll_probs.append(predictions_lda)\n",
        "all_probs.append(predictions_svm)\n",
        "all_probs.append(predictions_etc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rpmiX_X4JgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculated a weighted sum of predictions\n",
        "def weighted_sum(weights, yhats):\n",
        "\trows = list()\n",
        "\tfor j in range(yhats.shape[1]):\n",
        "\t\t# enumerate values\n",
        "\t\trow = list()\n",
        "\t\tfor k in range(yhats.shape[2]):\n",
        "\t\t\t# enumerate members\n",
        "\t\t\tvalue = 0.0\n",
        "\t\t\tfor i in range(yhats.shape[0]):\n",
        "\t\t\t\tvalue += weights[i] * yhats[i,j,k]\n",
        "\t\t\trow.append(value)\n",
        "\t\trows.append(row)\n",
        "\treturn array(rows)\n",
        "\n",
        "# make an ensemble prediction for binary classification\n",
        "def ensemble_predictions2(weights,all_probs):\n",
        "\t# make predictions\n",
        "\t#yhats = [model.predict(testX) for model in members]\n",
        "\tyhats = all_probs \n",
        "\tyhats = np.array(yhats)\n",
        "\t# weighted sum across ensemble members\n",
        "\tsummed = np.tensordot(yhats, weights, axes=((0),(0)))\n",
        "\t# argmax across classes\n",
        "\tresult = summed\n",
        "\treturn result\n",
        "\n",
        "\n",
        "# evaluate a specific number of members in an ensemble\n",
        "def evaluate_ensemble(weights,test_Y,all_probs):\n",
        "\t# make prediction\n",
        "\tyhat = ensemble_predictions2(weights,all_probs)\n",
        "\tlabels = (yhat > 0.5).astype(np.int)\n",
        "\tfpr, tpr, _ = roc_curve(np.array(test_Y).flatten(), np.array(yhat).flatten(), pos_label=1)\n",
        "\troc_auc = auc(fpr, tpr)\n",
        "\t# calculate accuracy\n",
        "\t#r2(np.array(test_Y).flatten(),np.array(yhat).flatten())\n",
        "\treturn roc_auc\n",
        "\n",
        "def normalize(weights):\n",
        "\t# calculate l1 vector norm\n",
        "\tresult = norm(weights, 1)\n",
        "\t# check for a vector of all zeros\n",
        "\tif result == 0.0:\n",
        "\t\treturn weights\n",
        "\t# return normalized vector (unit norm)\n",
        "\treturn weights / result\n",
        "\n",
        "# grid search weights\n",
        "def grid_search(test_Y,all_probs):\n",
        "\t# define weights to consider\n",
        "\tw = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\tbest_score, best_weights = 0.0, None\n",
        "\t# iterate all possible combinations (cartesian product)\n",
        "\tfor weights in product(w, repeat=len(all_probs)):\n",
        "\t\t# skip if all weights are equal\n",
        "\t\tif len(set(weights)) == 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# hack, normalize weight vector\n",
        "\t\tweights = normalize(weights)\n",
        "\t\t# evaluate weights\n",
        "\t\tscore = evaluate_ensemble(weights,test_Y,all_probs)\n",
        "\t\tif score > best_score:\n",
        "\t\t\tbest_score, best_weights = score, weights\n",
        "\t\t\tprint('>%s %.3f' % (best_weights, best_score))\n",
        "\treturn list(best_weights)\n",
        " \n",
        "\n",
        "# grid search for coefficients in a weighted average ensemble for the blobs problem\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import tensordot\n",
        "from numpy.linalg import norm\n",
        "from itertools import product\n",
        "# grid search weights\n",
        "weights = grid_search(test_Y,all_probs)\n",
        "score = evaluate_ensemble(weights,test_Y,all_probs)\n",
        "print('Grid Search Weights: %s, Score: %.3f' % (weights, score))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
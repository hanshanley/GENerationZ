# -*- coding: utf-8 -*-
"""CHAR_TCN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WHYsnbUbGj0PN7cbrLXgfESKOpvM9_Ob
"""

## The file create a temporal convolutional network 
## with posiitional embedding

import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import sys 
import GANS.stcn as stcn

class EmbeddingSharedWeights(keras.layers.Layer):
  def __init__(self, vocab_size, embedding_dim,max_length):
    super(EmbeddingSharedWeights, self).__init__()
    self.vocab_size = vocab_size
    self.embedding_dim = embedding_dim
    self.max_length = max_length
    self.embed =  keras.layers.Embedding(input_dim=self.vocab_size, 
                                         output_dim=self.embedding_dim,
                                 embeddings_initializer='random_normal',
                                 input_length=max_length,
                                 trainable=True)

  def call(self, x):
    """Get token embeddings of x.
    x: An int64 tensor with shape [batch_size, length]
    embeddings: float32 tensor with shape [batch_size, length, embedding_size]
    """
    #with tf.name_scope("embedding"):
    embeddings = self.embed(x)
    return embeddings

  def linear(self, x):
    """
        Computes logits by running x through a linear layer.
        Args: x - A float32 tensor with shape [batch_size, length, hidden_size]
        Returns: float32 tensor with shape [batch_size, length, vocab_size].
    """
    #with tf.name_scope("presoftmax_linear"):
    batch_size = tf.shape(x)[0]
    length = tf.shape(x)[1]
    x = tf.reshape(x, [-1, self.embedding_dim])
    logits = tf.matmul(x, self.embed.weights, transpose_b=True)

    return tf.reshape(logits, [batch_size, length, self.vocab_size])

class CHAR_TCN(tf.keras.Model):
  def __init__(self, 
              vocab_size, n_channels, kernel_size, dropout,
              embedding_dim, 
              max_length,
              pad_index,
              emb_dropout=0.1): 
    super(CHAR_TCN,self).__init__()
    self.max_length = max_length
    self.embedding_dim = embedding_dim
    self.vocab_size = vocab_size

    ## Embed characters into appropriate space
    self.mask = keras.layers.Masking(pad_index)
    self.embedding = EmbeddingSharedWeights(self.vocab_size, 
                                            self.embedding_dim,
                                            self.max_length)

    self.drop = keras.layers.Dropout(rate = emb_dropout)

    self.TCN = stcn.TCN(n_channels,kernel_size=kernel_size, dropout=dropout)

  def call(self,x,training=True):
    x = self.mask(x)
    x = self.embedding(x)
    x = self.drop(x,training)
  
    x = self.TCN(x,training =training)
    x = self.embedding.linear(x)
    return x

